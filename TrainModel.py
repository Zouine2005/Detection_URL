import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Charger les caract√©ristiques extraites
df = pd.read_csv("dataset_features.csv")

# S√©parer les features (X) et les labels (y)
X = df.drop(columns=["label"])  # Supprimer la colonne label pour garder les features
y = df["label"]  # Label (0 = l√©gitime, 1 = phishing)

# Diviser en ensemble d'entra√Ænement (80%) et de test (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalisation des donn√©es
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("‚úÖ Donn√©es pr√©par√©es avec succ√®s !")

# Initialiser le mod√®le
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement
model.fit(X_train, y_train)

# Pr√©dire sur l'ensemble de test
y_pred = model.predict(X_test)

# Afficher la pr√©cision
accuracy = accuracy_score(y_test, y_pred)
print(f"‚úÖ Pr√©cision du mod√®le : {accuracy * 100:.2f}%")

# Afficher un rapport d√©taill√©
print("üîç Rapport de classification :\n", classification_report(y_test, y_pred))

# Sauvegarder le mod√®le et le scaler
joblib.dump(model, "model.pkl")
print("‚úÖ Mod√®le sauvegard√© sous model.pkl")

joblib.dump(scaler, "scaler.pkl")
print("‚úÖ Scaler sauvegard√© sous scaler.pkl")

# Sauvegarder l'ordre des features
feature_names = list(X.columns)
joblib.dump(feature_names, "feature_names.pkl")
print("‚úÖ Liste des features sauvegard√©e sous feature_names.pkl")
